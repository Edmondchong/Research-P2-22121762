{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyNF/u13DV9fLDCc7mXyKo6Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4KaNlF_fUgmO","executionInfo":{"status":"ok","timestamp":1734632530472,"user_tz":-480,"elapsed":24358,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}},"outputId":"12ac0aa0-de56-417a-8f86-f97caa035aed"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers pandas sentence-transformers spacy"],"metadata":{"id":"78LgHt0NUqMo","executionInfo":{"status":"ok","timestamp":1734632533168,"user_tz":-480,"elapsed":2699,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"0b312bae-8237-4dad-fcc2-20c086563106"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.10/dist-packages (3.3.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.27.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (2.5.1+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.6.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (1.13.1)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers) (11.0.0)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.11)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n","Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.5.0)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n","Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.15.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.10.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (75.1.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.5.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n","Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.12.14)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n","Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n","Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n","Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n","Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n","Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (3.0.2)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers) (3.5.0)\n","Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"]}]},{"cell_type":"code","source":["import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","import spacy\n","\n","# Load spaCy model for NER\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Load LLM response\n","llm_response_df = pd.read_csv('/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/With RAG/RAG-Llama.csv')\n","\n","# Load reference texts\n","reference_files = {\n","    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\n","    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\n","    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\n","}\n","\n","reference_texts = {}\n","for source, file_path in reference_files.items():\n","    reference_texts[source] = pd.read_csv(file_path)\n","\n","# Initialize a sentence transformer model for semantic similarity\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to extract named entities\n","def extract_entities(text):\n","    doc = nlp(text)\n","    return set([ent.text for ent in doc.ents])\n","\n","# Function to compare LLM response with reference texts\n","def fact_check(llm_response, reference_texts):\n","    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\n","    llm_entities = extract_entities(llm_response)\n","    similarities = {}\n","    missing_points = {}\n","    extra_points = {}\n","\n","    for source, texts in reference_texts.items():\n","        max_similarity = 0\n","        reference_entities = set()\n","        for text in texts['cause']:\n","            ref_embedding = model.encode(text, convert_to_tensor=True)\n","            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                reference_entities = extract_entities(text)\n","        similarities[source] = max_similarity\n","        missing_points[source] = reference_entities - llm_entities\n","        extra_points[source] = llm_entities - reference_entities\n","\n","    return similarities, missing_points, extra_points\n","\n","# Evaluate all rows in the LLM response DataFrame\n","results = []\n","for index, row in llm_response_df.iterrows():\n","    llm_response = row['Cause']\n","    similarities, missing_points, extra_points = fact_check(llm_response, reference_texts)\n","    results.append({\n","        'Index': index,\n","        'LLM Response': llm_response,\n","        'MayoClinic Similarity': similarities.get('MayoClinic', 0),\n","        'Healthline Similarity': similarities.get('Healthline', 0),\n","        'MedlinePlus Similarity': similarities.get('MedlinePlus', 0),\n","        'MayoClinic Missing Points': ', '.join(missing_points.get('MayoClinic', set())),\n","        'Healthline Missing Points': ', '.join(missing_points.get('Healthline', set())),\n","        'MedlinePlus Missing Points': ', '.join(missing_points.get('MedlinePlus', set())),\n","        'MayoClinic Extra Points': ', '.join(extra_points.get('MayoClinic', set())),\n","        'Healthline Extra Points': ', '.join(extra_points.get('Healthline', set())),\n","        'MedlinePlus Extra Points': ', '.join(extra_points.get('MedlinePlus', set()))\n","    })\n","\n","# Convert results to a DataFrame and save\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('RAG-Llama_fact_check.csv', index=False)\n","\n","print(\"Results for all responses saved to Gemma_fact_check.csv\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FnvF-od75DWu","executionInfo":{"status":"ok","timestamp":1734633118490,"user_tz":-480,"elapsed":79895,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}},"outputId":"9f28334a-92ea-46aa-8b91-97ab9e7a915a"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/spacy/util.py:1740: UserWarning: [W111] Jupyter notebook detected: if using `prefer_gpu()` or `require_gpu()`, include it in the same cell right before `spacy.load()` to ensure that the model is loaded on the correct device. More information: http://spacy.io/usage/v3#jupyter-notebook-gpu\n","  warnings.warn(Warnings.W111)\n"]},{"output_type":"stream","name":"stdout","text":["Results for all responses saved to Gemma_fact_check.csv\n"]}]},{"cell_type":"markdown","source":["# Factual accuracy"],"metadata":{"id":"BM6VbPJfgCv2"}},{"cell_type":"code","source":["\"\"\"\n","import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","import spacy\n","\n","# Load spaCy model for NER\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Load LLM response\n","llm_response_df = pd.read_csv('/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/MixtralAI_advice.csv')\n","\n","# Load reference texts\n","reference_files = {\n","    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\n","    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\n","    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\n","}\n","\n","reference_texts = {}\n","for source, file_path in reference_files.items():\n","    reference_texts[source] = pd.read_csv(file_path)\n","\n","# Initialize a sentence transformer model for semantic similarity\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to extract named entities\n","def extract_entities(text):\n","    doc = nlp(text)\n","    return set([ent.text for ent in doc.ents])\n","\n","# Function to compare LLM response with reference texts\n","def fact_check(llm_response, reference_texts):\n","    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\n","    llm_entities = extract_entities(llm_response)\n","    similarities = {}\n","    entity_matches = {}\n","    missing_points = {}\n","    extra_points = {}\n","\n","    for source, texts in reference_texts.items():\n","        max_similarity = 0\n","        matched_entities = set()\n","        for text in texts['cause']:\n","            ref_embedding = model.encode(text, convert_to_tensor=True)\n","            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                matched_entities = extract_entities(text).intersection(llm_entities)\n","                missing_points[source] = extract_entities(text) - llm_entities\n","                extra_points[source] = llm_entities - extract_entities(text)\n","        similarities[source] = max_similarity\n","        entity_matches[source] = matched_entities\n","\n","    return similarities, entity_matches, missing_points, extra_points\n","\n","# Evaluate all rows in the LLM response DataFrame\n","results = []\n","for index, row in llm_response_df.iterrows():\n","    llm_response = row['Cause']\n","    similarities, entity_matches, missing_points, extra_points = fact_check(llm_response, reference_texts)\n","    results.append({\n","        'Index': index,\n","        'LLM Response': llm_response,\n","        'Similarities': similarities,\n","        'Entity Matches': entity_matches,\n","        'Missing Points': missing_points,\n","        'Extra Points': extra_points\n","    })\n","\n","# Convert results to a DataFrame and save\n","results_df = pd.DataFrame(results)\n","results_df.to_csv('Gemma_fact_check.csv', index=False)\n","\n","print(\"Results for all responses saved to fact_check_all_results.csv\")\n","\"\"\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"id":"H3ikhGLLaab5","executionInfo":{"status":"ok","timestamp":1734632533168,"user_tz":-480,"elapsed":7,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}},"outputId":"99a899a4-66d7-4167-fdcb-5185965571b3"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\nimport pandas as pd\\nimport numpy as np\\nfrom transformers import pipeline\\nfrom sentence_transformers import SentenceTransformer, util\\nimport spacy\\n\\n# Load spaCy model for NER\\nnlp = spacy.load(\\'en_core_web_sm\\')\\n\\n# Load LLM response\\nllm_response_df = pd.read_csv(\\'/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/MixtralAI_advice.csv\\')\\n\\n# Load reference texts\\nreference_files = {\\n    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\\n    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\\n    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\\n}\\n\\nreference_texts = {}\\nfor source, file_path in reference_files.items():\\n    reference_texts[source] = pd.read_csv(file_path)\\n\\n# Initialize a sentence transformer model for semantic similarity\\nmodel = SentenceTransformer(\\'all-MiniLM-L6-v2\\')\\n\\n# Function to extract named entities\\ndef extract_entities(text):\\n    doc = nlp(text)\\n    return set([ent.text for ent in doc.ents])\\n\\n# Function to compare LLM response with reference texts\\ndef fact_check(llm_response, reference_texts):\\n    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\\n    llm_entities = extract_entities(llm_response)\\n    similarities = {}\\n    entity_matches = {}\\n    missing_points = {}\\n    extra_points = {}\\n\\n    for source, texts in reference_texts.items():\\n        max_similarity = 0\\n        matched_entities = set()\\n        for text in texts[\\'cause\\']:\\n            ref_embedding = model.encode(text, convert_to_tensor=True)\\n            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\\n            if similarity > max_similarity:\\n                max_similarity = similarity\\n                matched_entities = extract_entities(text).intersection(llm_entities)\\n                missing_points[source] = extract_entities(text) - llm_entities\\n                extra_points[source] = llm_entities - extract_entities(text)\\n        similarities[source] = max_similarity\\n        entity_matches[source] = matched_entities\\n\\n    return similarities, entity_matches, missing_points, extra_points\\n\\n# Evaluate all rows in the LLM response DataFrame\\nresults = []\\nfor index, row in llm_response_df.iterrows():\\n    llm_response = row[\\'Cause\\']\\n    similarities, entity_matches, missing_points, extra_points = fact_check(llm_response, reference_texts)\\n    results.append({\\n        \\'Index\\': index,\\n        \\'LLM Response\\': llm_response,\\n        \\'Similarities\\': similarities,\\n        \\'Entity Matches\\': entity_matches,\\n        \\'Missing Points\\': missing_points,\\n        \\'Extra Points\\': extra_points\\n    })\\n\\n# Convert results to a DataFrame and save\\nresults_df = pd.DataFrame(results)\\nresults_df.to_csv(\\'Gemma_fact_check.csv\\', index=False)\\n\\nprint(\"Results for all responses saved to fact_check_all_results.csv\")\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["\"\"\"\n","# Install necessary libraries\n","!pip install transformers pandas sentence-transformers spacy\n","\n","import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","import spacy\n","\n","# Load spaCy model for NER\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Load LLM response\n","llm_response_df = pd.read_csv('/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/Llama_advice.csv')\n","\n","# Load reference texts\n","reference_files = {\n","    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\n","    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\n","    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\n","}\n","\n","reference_texts = {}\n","for source, file_path in reference_files.items():\n","    reference_texts[source] = pd.read_csv(file_path)\n","\n","# Initialize a sentence transformer model for semantic similarity\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to extract named entities\n","def extract_entities(text):\n","    doc = nlp(text)\n","    return set([ent.text for ent in doc.ents])\n","\n","# Function to compare LLM response with reference texts\n","def fact_check(llm_response, reference_texts):\n","    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\n","    llm_entities = extract_entities(llm_response)\n","    similarities = {}\n","    entity_matches = {}\n","    missing_points = {}\n","    extra_points = {}\n","\n","    for source, texts in reference_texts.items():\n","        max_similarity = 0\n","        matched_entities = set()\n","        for text in texts['cause']:\n","            ref_embedding = model.encode(text, convert_to_tensor=True)\n","            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                matched_entities = extract_entities(text).intersection(llm_entities)\n","                missing_points[source] = extract_entities(text) - llm_entities\n","                extra_points[source] = llm_entities - extract_entities(text)\n","        similarities[source] = max_similarity\n","        entity_matches[source] = matched_entities\n","\n","    return similarities, entity_matches, missing_points, extra_points\n","\n","# Example usage\n","llm_response = llm_response_df.iloc[0]['Cause']\n","similarities, entity_matches, missing_points, extra_points = fact_check(llm_response, reference_texts)\n","\n","print(f\"LLM Response: {llm_response}\")\n","print(f\"Similarities: {similarities}\")\n","print(f\"Entity Matches: {entity_matches}\")\n","print(f\"Missing Points: {missing_points}\")\n","print(f\"Extra Points: {extra_points}\")\n","\n","# Save results to CSV file\n","results_df = pd.DataFrame({\n","    'Source': list(similarities.keys()),\n","    'Similarity': list(similarities.values()),\n","    'Entity Matches': [', '.join(entities) for entities in entity_matches.values()],\n","    'Missing Points': [', '.join(points) for points in missing_points.values()],\n","    'Extra Points': [', '.join(points) for points in extra_points.values()]\n","})\n","\n","results_df.to_csv('fact_check_results.csv', index=False)\n","\n","print(\"Results saved to fact_check_results.csv\")\n","\"\"\""],"metadata":{"id":"y747dwFignTQ","executionInfo":{"status":"aborted","timestamp":1734632553748,"user_tz":-480,"elapsed":5,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","import spacy\n","\n","# Load spaCy model for NER\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Load LLM response\n","llm_response_df = pd.read_csv('/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/Llama_advice.csv')\n","\n","# Load reference texts\n","reference_files = {\n","    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\n","    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\n","    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\n","}\n","\n","reference_texts = {}\n","for source, file_path in reference_files.items():\n","    reference_texts[source] = pd.read_csv(file_path)\n","\n","# Initialize a sentence transformer model for semantic similarity\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to extract named entities\n","def extract_entities(text):\n","    doc = nlp(text)\n","    return set([ent.text for ent in doc.ents])\n","\n","# Function to compare LLM response with reference texts and assign factual accuracy marks\n","def fact_check(llm_response, reference_texts):\n","    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\n","    llm_entities = extract_entities(llm_response)\n","    similarities = {}\n","    entity_matches = {}\n","    missing_points = {}\n","    extra_points = {}\n","    factual_accuracy_marks = {}\n","\n","    for source, texts in reference_texts.items():\n","        max_similarity = 0\n","        matched_entities = set()\n","        for text in texts['cause']:\n","            ref_embedding = model.encode(text, convert_to_tensor=True)\n","            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                matched_entities = extract_entities(text).intersection(llm_entities)\n","                missing_points[source] = extract_entities(text) - llm_entities\n","                extra_points[source] = llm_entities - extract_entities(text)\n","        similarities[source] = max_similarity\n","        entity_matches[source] = matched_entities\n","\n","        # Calculate factual accuracy marks\n","        marks = max_similarity * 100  # Base similarity score out of 100\n","        marks += len(matched_entities) * 5  # Add points for each matched entity\n","        marks -= len(missing_points[source]) * 5  # Deduct points for each missing entity\n","        marks -= len(extra_points[source]) * 5  # Deduct points for each extra entity\n","        factual_accuracy_marks[source] = marks\n","\n","    return similarities, entity_matches, missing_points, extra_points, factual_accuracy_marks\n","\n","# Example usage\n","llm_response = llm_response_df.iloc[0]['Cause']\n","similarities, entity_matches, missing_points, extra_points, factual_accuracy_marks = fact_check(llm_response, reference_texts)\n","\n","print(f\"LLM Response: {llm_response}\")\n","print(f\"Similarities: {similarities}\")\n","print(f\"Entity Matches: {entity_matches}\")\n","print(f\"Missing Points: {missing_points}\")\n","print(f\"Extra Points: {extra_points}\")\n","print(f\"Factual Accuracy Marks: {factual_accuracy_marks}\")\n","\n","# Save results to CSV file\n","results_df = pd.DataFrame({\n","    'Source': list(similarities.keys()),\n","    'Similarity': list(similarities.values()),\n","    'Entity Matches': [', '.join(entities) for entities in entity_matches.values()],\n","    'Missing Points': [', '.join(points) for points in missing_points.values()],\n","    'Hallucinated Points': [', '.join(points) for points in extra_points.values()],\n","    'Factual Accuracy Marks': list(factual_accuracy_marks.values())\n","})\n","\n","results_df.to_csv('fact_check_results.csv', index=False)\n","\n","print(\"Results saved to fact_check_results.csv\")\n","\"\"\""],"metadata":{"id":"OqNnxyUMiJkz","executionInfo":{"status":"aborted","timestamp":1734632553748,"user_tz":-480,"elapsed":4,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","import pandas as pd\n","import numpy as np\n","from transformers import pipeline\n","from sentence_transformers import SentenceTransformer, util\n","import spacy\n","\n","# Load spaCy model for NER\n","nlp = spacy.load('en_core_web_sm')\n","\n","# Load LLM response\n","llm_response_df = pd.read_csv('/content/drive/MyDrive/P2/LLM/LLL-Advice/Results/Llama_advice.csv')\n","\n","# Load reference texts\n","reference_files = {\n","    \"MayoClinic\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MayoClinic.csv\",\n","    \"Healthline\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/Healthline.csv\",\n","    \"MedlinePlus\": \"/content/drive/MyDrive/P2/LLM/LLL-Advice/sources/MedlinePlus.csv\",\n","}\n","\n","reference_texts = {}\n","for source, file_path in reference_files.items():\n","    reference_texts[source] = pd.read_csv(file_path)\n","\n","# Initialize a sentence transformer model for semantic similarity\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","# Function to extract named entities\n","def extract_entities(text):\n","    doc = nlp(text)\n","    return set([ent.text for ent in doc.ents])\n","\n","# Function to compare LLM response with reference texts and calculate factual accuracy\n","def fact_check(llm_response, reference_texts):\n","    llm_embedding = model.encode(llm_response, convert_to_tensor=True)\n","    llm_entities = extract_entities(llm_response)\n","    similarities = {}\n","    entity_matches = {}\n","    missing_points = {}\n","    extra_points = {}\n","    factual_accuracy = {}\n","\n","    for source, texts in reference_texts.items():\n","        max_similarity = 0\n","        matched_entities = set()\n","        reference_entities = set()\n","\n","        for text in texts['cause']:\n","            ref_embedding = model.encode(text, convert_to_tensor=True)\n","            similarity = util.pytorch_cos_sim(llm_embedding, ref_embedding).item()\n","            if similarity > max_similarity:\n","                max_similarity = similarity\n","                matched_entities = extract_entities(text).intersection(llm_entities)\n","                reference_entities.update(extract_entities(text))\n","\n","        missing_points[source] = reference_entities - llm_entities\n","        extra_points[source] = llm_entities - reference_entities\n","\n","        # Calculate factual accuracy\n","        total_reference_points = len(reference_entities)\n","        factual_accuracy[source] = ((total_reference_points - len(missing_points[source])) / total_reference_points) * 100 if total_reference_points > 0 else 0\n","\n","        similarities[source] = max_similarity\n","        entity_matches[source] = matched_entities\n","\n","    return similarities, entity_matches, missing_points, extra_points, factual_accuracy\n","\n","# Example usage\n","llm_response = llm_response_df.iloc[0]['Cause']\n","similarities, entity_matches, missing_points, extra_points, factual_accuracy = fact_check(llm_response, reference_texts)\n","\n","print(f\"LLM Response: {llm_response}\")\n","print(f\"Similarities: {similarities}\")\n","print(f\"Entity Matches: {entity_matches}\")\n","print(f\"Missing Points: {missing_points}\")\n","print(f\"Extra Points: {extra_points}\")\n","print(f\"Factual Accuracy: {factual_accuracy}\")\n","\n","# Save results to CSV file\n","results_df = pd.DataFrame({\n","    'Source': list(similarities.keys()),\n","    'Similarity': list(similarities.values()),\n","    'Entity Matches': [', '.join(entities) for entities in entity_matches.values()],\n","    'Missing Points': [', '.join(points) for points in missing_points.values()],\n","    'Extra Points': [', '.join(points) for points in extra_points.values()],\n","    'Factual Accuracy (%)': list(factual_accuracy.values())\n","})\n","\n","results_df.to_csv('fact_check_results.csv', index=False)\n","\n","print(\"Results saved to fact_check_results.csv\")\n","\"\"\""],"metadata":{"id":"PCanwFZzcMfZ","executionInfo":{"status":"aborted","timestamp":1734632553748,"user_tz":-480,"elapsed":4,"user":{"displayName":"Edmond Chong","userId":"00827124523005702132"}}},"execution_count":null,"outputs":[]}]}